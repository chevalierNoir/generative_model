{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from ps4_utils import load_data,load_experiment\n",
    "from ps4_utils import AbstractGenerativeModel\n",
    "from ps4_utils import save_submission\n",
    "from scipy.misc import logsumexp\n",
    "import numpy as np\n",
    "data_fn = \"datasets-ps4.h5\"\n",
    "MAX_OUTER_ITER = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MixtureModel(AbstractGenerativeModel):\n",
    "    def __init__(self, CLASSES, NUM_FEATURES, NUM_MIXTURE_COMPONENTS, MAX_ITER=50, EPS=10**(-7)):\n",
    "        AbstractGenerativeModel.__init__(self, CLASSES, NUM_FEATURES)\n",
    "        self.num_mixture_components = NUM_MIXTURE_COMPONENTS # list of num_mixture_components (length num_classes)\n",
    "        self.max_iter = MAX_ITER # max iterations of EM\n",
    "        self.epsilon = EPS # help with stability, to be used according to hint given at end of pset4.pdf\n",
    "        self.params = { # lists of length CLASSES\n",
    "            'pi': [np.repeat(1.0/k,k) for k in self.num_mixture_components], # with pi_c for each class\n",
    "            # Original: np.zeros()\n",
    "            'theta': [np.random.rand(self.num_features,k) for k in self.num_mixture_components], # with theta_c for each class\n",
    "        }\n",
    "    def pack_params(self, X, class_idx):\n",
    "        pi,theta = self.fit(X[class_idx],class_idx) # fit parameters\n",
    "        self.params['pi'][class_idx] = pi # update member variable pi\n",
    "        self.params['theta'][class_idx] = theta #update member variable theta\n",
    "        \n",
    "    #make classification based on which mixture model gives higher probability to generating point xi\n",
    "    def classify(self, X):\n",
    "        P = list()\n",
    "        pi = self.params['pi']\n",
    "        theta = self.params['theta']\n",
    "        for c in range(self.num_classes):\n",
    "            _,Pc = self.findP(X, pi[c], theta[c])\n",
    "            P.append(Pc)\n",
    "        return np.vstack(P).T.argmax(-1) # np.array of class predictions for each data point in X\n",
    "\n",
    "    # --- E-step\n",
    "    def updateLatentPosterior(self, X, pi, theta, num_mixture_components): # update the latent posterior\n",
    "        # YOUR CODE HERE\n",
    "        # --- gamma: np.array (matrix)\n",
    "        # ---        shape: number of data points in X (where X consists of datapoints from class c) by NUM_MIXTURE_COMPONENTS[c]\n",
    "        N = X.shape[0]\n",
    "        gamma = np.zeros([N, num_mixture_components])\n",
    "#         for i in range(N):\n",
    "#             log_A = np.zeros([num_mixture_components])\n",
    "#             for c in range(num_mixture_components):\n",
    "#                 log_A[c] = np.dot(X[i], np.log(theta.T[c])) + np.dot(1 - X[i], np.log(1-theta[c]))\n",
    "#             A = np.exp(log_A)/np.sum(np.exp(log_A))\n",
    "#             gamma[i] = A\n",
    "        for c in range(num_mixture_components):\n",
    "            theta_c = theta[:, c].clip(min = 1e-7, max = 1.0 - 1e-7)\n",
    "#             theta_c = theta[:, c]\n",
    "            log_gamma = np.log(np.repeat(pi[c], N)) + np.dot(X, np.log(theta_c)) + np.dot(1.0 - X, np.log(1 - theta_c))\n",
    "            gamma[:, c] = log_gamma\n",
    "        gamma = gamma - np.mean(gamma)\n",
    "        gamma = np.exp(gamma)\n",
    "        gamma = np.divide(gamma, np.repeat(np.sum(gamma, axis=1).reshape([N, 1]), num_mixture_components, axis=1))\n",
    "#         for i in range(N):\n",
    "#             s = np.sum(gamma[i, :])\n",
    "#             if s - 1.0 >= 1.0e-5 or s - 1.0 <= -1.0e-5:\n",
    "#                 print(\"gamma line sum not zero: line {}, sum: {}\".format(i, s))\n",
    "        return gamma\n",
    "    # --- M-step (1)\n",
    "    @staticmethod\n",
    "    def updatePi(gamma): #update the pi component using the posteriors (gammas)\n",
    "        # YOUR CODE HERE\n",
    "        # --- pi_c: class specific pi, np.array (vector)\n",
    "        # ---        shape: NUM_MIXTURE_COMPONENTS[c]\n",
    "        pi_c = np.sum(gamma, axis=0)\n",
    "        return pi_c\n",
    "    # -- M-step (2)\n",
    "    @staticmethod\n",
    "    def updateTheta(X, gamma): #update theta component using posteriors (gammas)\n",
    "        # YOUR CODE HERE\n",
    "        # --- theta_c: class specific theta, np.array matrix\n",
    "        # ---        shape: NUM_FEATURES by NUM_MIXTURE_COMPONENTS[c]\n",
    "        num_mixture_components = gamma.shape[1]\n",
    "        num_features = X.shape[1]\n",
    "        theta_c = np.zeros([num_features, num_mixture_components])\n",
    "        for c in range(num_mixture_components):\n",
    "            gamma_c = gamma[:, c]\n",
    "            theta_c[:,c] = np.dot(X.T, gamma_c)/np.sum(gamma_c)\n",
    "        return theta_c \n",
    "    \n",
    "    @staticmethod\n",
    "    def findP(X, pi, theta):\n",
    "        # YOUR CODE HERE\n",
    "        # --- t: probabilities of x given each component of mixture\n",
    "        # ---        shape: number of data points in X (where X consists of datapoints from class c) by NUM_MIXTURE_COMPONENTS[c] \n",
    "        # --- logsumexp(t,axis=1): normalized by factor of probabilities of x over all components of mixture\n",
    "        # ---        shape: number of data points in X (where X consists of datapoints from class c)\n",
    "        \n",
    "        # Use log-probability for t here\n",
    "        k = theta.shape[1]\n",
    "        N = X.shape[0]\n",
    "        t = np.zeros([N, k])\n",
    "        for l in range(k):\n",
    "            t[:,l] = np.log(np.repeat(pi[l], N)) + np.dot(X, np.log(theta[:,l])) + np.dot(1 - X, np.log(1 - theta[:,l]))\n",
    "        return t,logsumexp(t,axis=1)\n",
    "        \n",
    "    # --- execute EM procedure\n",
    "    def fit(self, X, class_idx):\n",
    "        max_iter = self.max_iter\n",
    "        eps = self.epsilon\n",
    "        N = X.shape[0]\n",
    "        pi = self.params['pi'][class_idx]\n",
    "        theta = self.params['theta'][class_idx]\n",
    "        num_mixture_components = self.num_mixture_components[class_idx]\n",
    "        # INITIALIZE theta\n",
    "        for i in range(max_iter):\n",
    "            # YOUR CODE HERE, E-step: gamma = self.updateLatentPosterior\n",
    "            # YOUR CODE HERE, M-step(1): pi = self.updatePi \n",
    "            # YOUR CODE HERE, M-step(2): theta = self.updateTheta\n",
    "            gamma = self.updateLatentPosterior(X, pi, theta, num_mixture_components)\n",
    "            pi = self.updatePi(gamma)\n",
    "            theta = self.updateTheta(X, gamma)\n",
    "        return pi,theta #pi and theta, given class_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class NaiveBayesModel(AbstractGenerativeModel):\n",
    "    def __init__(self, CLASSES, NUM_FEATURES, EPS=10**(-12)):\n",
    "        AbstractGenerativeModel.__init__(self, CLASSES, NUM_FEATURES)\n",
    "        self.epsilon = EPS # help with stability\n",
    "        self.params = {\n",
    "            'logp': [np.zeros((NUM_FEATURES))] * self.num_classes # estimated log-probabilities\n",
    "        }\n",
    "    def pack_params(self, X, class_idx):\n",
    "        logp = self.fit(X[class_idx])\n",
    "        self.params['logp'][class_idx] = logp\n",
    "    def classify(self, X): # naive bayes classifier\n",
    "        # YOUR CODE HERE\n",
    "        logp = self.params['logp']\n",
    "        predictions = np.dot(logp, X.T)\n",
    "        predictions = np.argmax(predictions, axis = 0)\n",
    "        return predictions\n",
    "    def fit(self, X): \n",
    "        # YOUR CODE HERE\n",
    "        estimated_logp = np.log(np.mean(X, axis = 0) + self.epsilon)\n",
    "        return estimated_logp\n",
    "    \n",
    "#     def val(self, X, acc=0, N=0):\n",
    "#         print self.num_classes\n",
    "#         for c in range(self.num_classes):\n",
    "#             predictions = self.classify(X[c])\n",
    "# #             print('predictions: {}, c: {}'.format(predictions, c))\n",
    "#             print(np.sum(predictions))\n",
    "#             acc += np.sum((predictions == c).astype(np.int32))\n",
    "#             N += X[c].shape[0]\n",
    "#         return (acc / float(N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SENTIMENT ANALYSIS -- NAIVE BAYES MODEL:\n",
      "ACCURACY ON VALIDATION: 0.72\n",
      "SENTIMENT ANALYSIS -- MIXTURE MODEL:\n",
      "COMPONENTS: 5 8\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-117-bf931cbbe088>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"COMPONENTS: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnum_mixture_components\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mmm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMixtureModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_mixture_components\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mmm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ACCURACY ON VALIDATION: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/bowenshi/courses/intro_ml/ps4/ps4_utils.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-115-fd2b956811fc>\u001b[0m in \u001b[0;36mpack_params\u001b[0;34m(self, X, class_idx)\u001b[0m\n\u001b[1;32m     11\u001b[0m         }\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpack_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mpi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtheta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclass_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclass_idx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# fit parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pi'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclass_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpi\u001b[0m \u001b[0;31m# update member variable pi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'theta'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclass_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtheta\u001b[0m \u001b[0;31m#update member variable theta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-115-fd2b956811fc>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, class_idx)\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[0;31m# YOUR CODE HERE, M-step(1): pi = self.updatePi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0;31m# YOUR CODE HERE, M-step(2): theta = self.updateTheta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m             \u001b[0mgamma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdateLatentPosterior\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_mixture_components\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m             \u001b[0mpi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdatePi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0mtheta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdateTheta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-115-fd2b956811fc>\u001b[0m in \u001b[0;36mupdateLatentPosterior\u001b[0;34m(self, X, pi, theta, num_mixture_components)\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0mtheta_c\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e-7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1e-7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;31m#             theta_c = theta[:, c]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0mlog_gamma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta_c\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtheta_c\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m             \u001b[0mgamma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlog_gamma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mgamma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgamma\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "experiment_name = \"sentiment_analysis\"\n",
    "# --- SENTIMENT ANALYSIS setup\n",
    "Xtrain,Xval,num_classes,num_features = load_experiment(data_fn, experiment_name)\n",
    "\n",
    "# -- build naive bayes model for sentiment analysis\n",
    "print(\"SENTIMENT ANALYSIS -- NAIVE BAYES MODEL:\")\n",
    "nbm = NaiveBayesModel(num_classes, num_features)\n",
    "nbm.train(Xtrain)\n",
    "print(\"ACCURACY ON VALIDATION: \" + str(nbm.val(Xval)))\n",
    "\n",
    "# -- build mixture model for sentiment analysis\n",
    "print(\"SENTIMENT ANALYSIS -- MIXTURE MODEL:\")\n",
    "for i in range(MAX_OUTER_ITER):\n",
    "    num_mixture_components =  np.random.randint(2,15,num_classes)\n",
    "    print(\"COMPONENTS: \" + \" \".join(str(i) for i in num_mixture_components))\n",
    "    mm = MixtureModel(num_classes, num_features, num_mixture_components)\n",
    "    mm.train(Xtrain)\n",
    "    print(\"ACCURACY ON VALIDATION: \" + str(mm.val(Xval)))\n",
    "\n",
    "# submit to kaggle\n",
    "Xkaggle = load_data(data_fn, experiment_name, \"kaggle\")\n",
    "save_submission(\"nb-{}-submission.csv\".format(experiment_name), nbm.classify(Xkaggle))\n",
    "save_submission(\"mm-{}-submission.csv\".format(experiment_name), mm.classify(Xkaggle))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST DIGIT CLASSIFICATION -- NAIVE BAYES MODEL:\n",
      "ACCURACY ON VALIDATION: 0.216\n",
      "MNIST DIGIT CLASSIFICATION -- MIXTURE MODEL:\n",
      "COMPONENTS: 2 11 2 8 12 5 3 9 11 7\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'MixtureModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-e113594842bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mnum_mixture_components\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"COMPONENTS: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnum_mixture_components\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mmm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMixtureModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_mixture_components\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mmm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ACCURACY ON VALIDATION: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'MixtureModel' is not defined"
     ]
    }
   ],
   "source": [
    "experiment_name = \"mnist\"\n",
    "# --- MNIST DIGIT CLASSIFICATION setup\n",
    "Xtrain,Xval,num_classes,num_features = load_experiment(data_fn, experiment_name)\n",
    "\n",
    "# -- build naive bayes model for mnist digit classification\n",
    "print(\"MNIST DIGIT CLASSIFICATION -- NAIVE BAYES MODEL:\")\n",
    "nbm = NaiveBayesModel(num_classes, num_features)\n",
    "nbm.train(Xtrain)\n",
    "print(\"ACCURACY ON VALIDATION: \" + str(nbm.val(Xval)))\n",
    "\n",
    "# -- build mixture model for mnist digit classification\n",
    "print(\"MNIST DIGIT CLASSIFICATION -- MIXTURE MODEL:\")\n",
    "for i in range(MAX_OUTER_ITER):\n",
    "    num_mixture_components =  np.random.randint(2,15,num_classes)\n",
    "    print(\"COMPONENTS: \" + \" \".join(str(i) for i in num_mixture_components))\n",
    "    mm = MixtureModel(num_classes, num_features, num_mixture_components)\n",
    "    mm.train(Xtrain)\n",
    "    print(\"ACCURACY ON VALIDATION: \" + str(mm.val(Xval)))\n",
    "    \n",
    "# submit to kaggle\n",
    "Xkaggle = load_data(data_fn, experiment_name, \"kaggle\")\n",
    "save_submission(\"nb-{}-submission.csv\".format(experiment_name), nbm.classify(Xkaggle))\n",
    "save_submission(\"mm-{}-submission.csv\".format(experiment_name), mm.classify(Xkaggle))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
