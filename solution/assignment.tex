\documentclass[10pt]{article}
\usepackage{fullpage}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{setspace}
\usepackage{float}
\usepackage{hyperref}
\doublespacing

\begin{document}

\title{Assignment}
\date{}

\maketitle

\section{Regularization}

  We first calculate the gradient of squared loss as follows.

\begin{equation}\label{eq1}
\bigtriangledown_{w}L_{1}=\displaystyle\sum_{i=1}^{N}2(y_{i}-w^{T}\phi(x_{i}))(-\phi(x_{i})) +2\lambda w = 0
\end{equation}

Using Lagrange multiplier, the second objective can be turned into following formula

\begin{equation}\label{eq2}
min\{L_{2}\} = min\{\displaystyle\sum_{i=1}^{N}(y_{i}-w^{T}\phi(x_{i}))^{2}+\alpha(\displaystyle\sum_{i=1}^{d}w_{j}^{2}-\tau)\}
\end{equation}

According to the KKT condition, the solution of $min\{L_{2}\}$ should satisfy the conditions 

\begin{equation}\label{eq3}
\bigtriangledown_{w}L_{2}=\displaystyle\sum_{i=1}^{N}2(y_{i}-w^{T}\phi(x_{i}))(-\phi(x_{i})) +2\alpha w = 0
\end{equation}
\begin{equation}\label{eq4}
\alpha(\displaystyle\sum_{j=1}^{d}w_{j}^{2}-\tau)=0
\end{equation}
\begin{equation}\label{eq5}
\alpha\geq 0
\end{equation}
\begin{equation}\label{eq6}
\displaystyle\sum_{j=1}^{d}w_{j}^{2}-\tau\leq 0
\end{equation}

Suppose $w^{\star}_{1}$ is the solution of \ref{eq1}, we now show the solution of \ref{eq2} is also $w^{\star}_{1}$ in two cases of $\lambda=0$ and $\lambda>0$.

1. $\lambda>0$. Let $\alpha=\lambda$, thus $w^{\star}_{1}$ is the solution of \ref{eq3}. Let $\tau=\displaystyle\sum_{j=1}^{d}(w^{\star}_{1j})^{2}$ and equation \ref{eq4}, \ref{eq5} and \ref{eq6} are satisfied. Therefore, $w^{\star}_{1}$ is the solution of $\ref{eq2}$.

2. $\lambda=0$. Let $\alpha=\lambda = 0$ and $w^{\star}_{1}$ is the solution of \ref{eq3}. Let $\tau\geq\displaystyle\sum_{j=1}^{d}(w^{\star}_{1j})^{2}$ and equation \ref{eq4}, \ref{eq5} and \ref{eq6} are satisfied. Therefore, $w^{\star}_{1}$ is the solution of $\ref{eq2}$.

According to the above, the solution of equation \ref{eq1} and \ref{eq2} are identical if the $\tau$(in equation \ref{eq2}) is chosen correctly. Therefore, the two objectives are same.

\section{Naive Bayes}
The Naive Bayes classifier is a function as follows:

\begin{equation}\label{eq_naive}
\begin{aligned}
& h^{\star}(x)=\operatorname*{arg\,max}_c \delta_{c}(x) \\
& \delta_{c}(x) = \text{log}\,p(x|y=c)
\end{aligned}
\end{equation}

Suppose the feature is $x=(x_{1},...,x_{n})^{T}$ and features are of Bernoulli distribution. $p(x|y=c)=\displaystyle\prod_{i=1}^{n}p_{ci}^{x_{i}}(1-p_{ci})^{1-x_{i}}$, where $p_{ci}$ is the probability of $i$th item in class $c$(we use frequency to approximate the probability). Therefore, the corresponding Naive Bayes classifier\ref{eq_naive} can be further transformed into classifier \ref{eq_bernoulli}.

\begin{equation}\label{eq_bernoulli}
\begin{aligned}
& h^{\star}(x)=\operatorname*{arg\,max}_c \delta_{c}(x) \\
& \delta_{c}(x) = \displaystyle\sum_{i=1}^{n}x_{i}\text{log}\,p_{ci}+(1-x_{i})\text{log}\,(1-p_{ci})
\end{aligned}
\end{equation}

%--BIBLIOGRAPHY--%
% \begin{thebibliography}{99}

% \bibitem{SampleBibLabel} Author, "Title",  Journal, Volume, Pages, Year. \\
% This is the annotation for this bibliographic record.

% \end{thebibliography}

\end{document}
